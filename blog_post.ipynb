{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to generate lyrics and music with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will train RNN character-level language model on lyrics dataset of\n",
    "most popular/recent artists.Having a trained model, we will sample a couple of\n",
    "songs which will be a funny mixture of different styles of different artists.\n",
    "After that we will update our model to become a Conditional Character-Level RNN,\n",
    "making it possible for us to sample songs conditioned on artist.\n",
    "And finally, we conclude by training our model on midi dataset of piano songs.\n",
    "While solving all these tasks, we will briefly explore some interesting concepts related to RNN\n",
    "training and inference like Character-Level RNN, Conditional Character-Level RNN,\n",
    "sampling from RNN, truncated backpropagation through time and checkpointing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character_level_model.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before choosing a model, let's have a closer look at our task. Given current letter and all previous\n",
    "letters, we will try to predict the next character. During training we will just take a sequence, and use\n",
    "all its characters except the last one as an input and the same sequence starting from the second character as groundtruth (See the picture). We will start from the simplest model that ignores all the previous characters while making a prediction, improve this model takes only a certain number of previous characters into account, and conclude with a model that takes all the previous characters into consideration while making a prediction.\n",
    "\n",
    "Our language model is defined on a character level. We will create a dictionary which will contain\n",
    "all English characters plus some special symbols, like period, comma, and end-of-line symbol. Each charecter will be represented as one-hot-encoded tensor. For more information about character-level models and examples, I recommend [this resource](https://github.com/spro/practical-pytorch). We could have\n",
    "used a more advanced word-level model, but we will keep it simple for now.\n",
    "\n",
    "Having characters, we can now form sequences of characters. We can generate sentences even now just by\n",
    "randomly sampling character after character with a fixed probability $p(any~letter)=\\frac{1}{dictionary~size}$.\n",
    "That's the most simple character level language model. Can we do better than this? Yes, we can compute the probabily of occurance of each letter from our training corpus (number of times a letter occures divided by the size of our dataset) and randomly sample letter using these probabilities. This model is better but it totally ignores the relative positional aspect of each letter. For example, pay attention on how you read any word: you start with the first letter, which is usually hard to predict, but as you reach the end of a word you can sometimes guess the next letter. When you read any word you are implicitly using some rules which you learned by reading other texts: for example, with each additional letter that you read from a word, the probability of a space character increases (really long words are rare) or the probability of any consonant after the letter \"r\" is low as it usually followed by vowel. There are lot of similar rules and we hope that our model will be able to learn them from data. To give our model a chance to learn these rules we need to extend it.\n",
    "\n",
    "Let's make a small gradual improvement of our model and let probability of each letter depend\n",
    "only on the previously occured letter ([markov assumption](https://en.wikipedia.org/wiki/Markov_property)). So, basically we will have $p(current~letter|previous~letter)$.\n",
    "This is a [Markov chain model](https://en.wikipedia.org/wiki/Markov_chain) (also try these [interactive visualizations](http://setosa.io/ev/markov-chains/) if you are not familiar with it). We can also estimate the probability distribution $p(current~letter|previous~letter)$ from our training dataset. This model is limited because in most cases the probability of the current letter depends not only on the previous letter.\n",
    "\n",
    "What we would like to model is actually $p(current~letter|all~previous~letters)$. At first, the task seems intractable as the number of previous letters is variable and it might become really large in case of long\n",
    "sequences. Turns out Reccurent Neural Netoworks can tackle this problem to a certain extent by using shared weights and fixed size hidden state. This leads us to a next section dedicated to RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](rnn_unfold.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are a family of neural networks for processing sequential data.\n",
    "Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs.\n",
    "Because of arbitrary size input sequences, they are concisely depicted as a graph with a cycle (See the picture).\n",
    "But they can be \"unfolded\" if the size of input sequence is known. They define a non-linear mapping from a current input $x_t$ and previous hidden state $s_{t-1}$ to the output $o_t$ and current hidden state $s_t$. Hidden state size has a predefined size and stores features which are updated on each step and affect the result of mapping.\n",
    "\n",
    "Now align the previous picture of the character-level language model and the ufolded RNN picture to see how\n",
    "we are using the RNN model to learn a character level language model.\n",
    "\n",
    "While the picture depicts the Vanilla RNN, we will use LSTM in our work as it is easier to train usually achieves better results.\n",
    "\n",
    "For a more elaborate introduction to RNNs, we refer reader to the [following resource](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiments we have chosen [55000+ Song Lyrics Kaggle dataset](https://www.kaggle.com/mousehead/songlyrics) which contains good variety of recent artists and more older ones. It is stored as a pandas file and we wrote a python wrapper around it to be able to use it for training purposes. You will have to download it yourself in order to be able to use our code.\n",
    "\n",
    "In order to be able to interpret the results better, I have chosen a subset of artists which I am\n",
    "more or less familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists = [\n",
    "'ABBA',\n",
    "'Ace Of Base',\n",
    "'Aerosmith',\n",
    "'Avril Lavigne',\n",
    "'Backstreet Boys',\n",
    "'Bob Marley',\n",
    "'Bon Jovi',\n",
    "'Britney Spears',\n",
    "'Bruno Mars',\n",
    "'Coldplay',\n",
    "'Def Leppard',\n",
    "'Depeche Mode',\n",
    "'Ed Sheeran',\n",
    "'Elton John',\n",
    "'Elvis Presley',\n",
    "'Eminem',\n",
    "'Enrique Iglesias',\n",
    "'Evanescence',\n",
    "'Fall Out Boy',\n",
    "'Foo Fighters',\n",
    "'Green Day',\n",
    " 'HIM',\n",
    " 'Imagine Dragons',\n",
    " 'Incubus',\n",
    " 'Jimi Hendrix',\n",
    " 'Justin Bieber',\n",
    " 'Justin Timberlake',\n",
    "'Kanye West',\n",
    " 'Katy Perry',\n",
    " 'The Killers',\n",
    " 'Kiss',\n",
    " 'Lady Gaga',\n",
    " 'Lana Del Rey',\n",
    " 'Linkin Park',\n",
    " 'Madonna',\n",
    " 'Marilyn Manson',\n",
    " 'Maroon 5',\n",
    " 'Metallica',\n",
    " 'Michael Bolton',\n",
    " 'Michael Jackson',\n",
    " 'Miley Cyrus',\n",
    " 'Nickelback',\n",
    " 'Nightwish',\n",
    " 'Nirvana',\n",
    " 'Oasis',\n",
    " 'Offspring',\n",
    " 'One Direction',\n",
    " 'Ozzy Osbourne',\n",
    " 'P!nk',\n",
    " 'Queen',\n",
    " 'Radiohead',\n",
    " 'Red Hot Chili Peppers',\n",
    " 'Rihanna',\n",
    " 'Robbie Williams',\n",
    " 'Rolling Stones',\n",
    " 'Roxette',\n",
    " 'Scorpions',\n",
    " 'Snoop Dogg',\n",
    " 'Sting',\n",
    " 'The Script',\n",
    " 'U2',\n",
    " 'Weezer',\n",
    " 'Yellowcard',\n",
    " 'ZZ Top']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training unconditional character-level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first experiment consisted of training of our character-level language model RNN\n",
    "on the whole corpus. We didn't take into consideration the artist information while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to sample a couple of songs after training our model. Basically, on each\n",
    "step our RNN will output logits and we can softmax them and sample from that distribution.\n",
    "Or we can use Gumble-Max trick and [sample using logits directly](https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/) which is equivalent.\n",
    "\n",
    "One intersting thing about sampling is that we can partially define the input sequence ourselves and start sampling\n",
    "with that initial condition. For example, we can sample a song that starts with \"Why\":\n",
    "\n",
    "```\n",
    "Why do you have to leave me?  \n",
    "I think I know I'm not the only one  \n",
    "I don't know if I'm gonna stay awake  \n",
    "I don't know why I go along  \n",
    "  \n",
    "I don't know why I can't go on  \n",
    "I don't know why I don't know  \n",
    "I don't know why I don't know  \n",
    "I don't know why I keep on dreaming of you   \n",
    "```\n",
    "\n",
    "Well, that sounds like a possible song :D\n",
    "\n",
    "Let's sample with a song that starts with \"Well\":\n",
    "\n",
    "```\n",
    "Well, I was a real good time  \n",
    "I was a rolling stone  \n",
    "I was a rock and roller  \n",
    "Well, I never had a rock and roll  \n",
    "There were times I had to do it  \n",
    "I had a feeling that I was found  \n",
    "I was the one who had to go  \n",
    "```\n",
    "\n",
    "There is \"temperature\" parameter that is used during sampling which controls the randomness of sampling\n",
    "process. It can take values between zero (not equal) and infinity. When this parameter approaches zero,\n",
    "the sampling is equivalent to argmax and when it is close to infinity the sampling is equivalent to sampling\n",
    "from a uniform distribution. Have a look at the figure from a [relevant paper](https://arxiv.org/pdf/1611.01144.pdf):\n",
    "\n",
    "\n",
    "![alt text](sampling_temperature.png \"Logo Title Text 1\")\n",
    "\n",
    "When $\\tau=1$, the distribution is not affected. If we decrease $\\tau$, the distribution\n",
    "becomes more pronounced, meaning that value with bigger probability mass will have it increased. When $\\tau$ will approach zero, sampling will be equivalent to armax, because the probability of that value will be close to one. When we start to icrease $\\tau$ the distribution becomes more and more uniform.\n",
    "\n",
    "The previous sample was generated with a temperature paramter equal to $0.5$.\n",
    "Let's see what happens when we increase it to $1.0$ and sample:\n",
    "\n",
    "```\n",
    "Why can't we drop out of time?  \n",
    "We were born for words to see.  \n",
    "Won't you love this. You're still so amazing.  \n",
    "This could be that down on Sunday Time.  \n",
    "Oh, Caroline, a lady floor.  \n",
    "I thought of love, oh baby.  \n",
    "```\n",
    "\n",
    "Let's try increasing it even more:\n",
    "\n",
    "\n",
    "```\n",
    "Why - won't we grow up naked?  \n",
    "We went quietly what we would've still give  \n",
    "That girl you walked before our bedroom room  \n",
    "I see your mind is so small to a freak  \n",
    "Stretching for a cold white-heart of crashing  \n",
    "Truth in the universal daughter  \n",
    "  \n",
    "I lose more and more hard  \n",
    "I love you anytime at all  \n",
    "Ah come let your help remind me  \n",
    "Now I've wanted waste and never noticed  \n",
    "  \n",
    "I swear I saw you today  \n",
    "You needed to get by  \n",
    "But you sold a hurricane  \n",
    "Well out whispered in store\n",
    "```\n",
    "\n",
    "Why don't we grow up naked, indeed? :D\n",
    "Well, you can see that trend that when we increase the temperature, sampled\n",
    "sentences become more and more random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training conditional character-level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine if we could generate lyrics in a style of some particular artist.\n",
    "Let's change our model, so that it can use this information during training.\n",
    "\n",
    "We will do this by adding an additional input to our RNN. So far, our RNN model\n",
    "was only accepting tensors containing one-hot encoded character on each step.\n",
    "\n",
    "The extention to our model will be very simple: we will have and additional one-hot encoded\n",
    "tensor which will represent the artist. So on each step the RNN will accept one tensor which will consist of concatenated tensors representing character and artist. Look [here for more](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from conditional language model RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we sampled a couple of songs conditined on artist.\n",
    "Below you can find some results.\n",
    "\n",
    "Him:\n",
    "\n",
    "```\n",
    "My fears  \n",
    "And the moment don't make me sing  \n",
    "So free from you  \n",
    "The pain you love me yeah  \n",
    "  \n",
    "Whatever caused the warmth  \n",
    "You smile you're happy  \n",
    "You sit away  \n",
    "You say it's all in vain  \n",
    "```\n",
    "\n",
    "Seems really possible, especially the fact the the word pain was used, which is\n",
    "very common in the lyrics of the artist.\n",
    "\n",
    "ABBA:\n",
    "\n",
    "```\n",
    "Oh, my love it makes me close a thing  \n",
    "You've been heard, I must have waited  \n",
    "I hear you  \n",
    "So I say  \n",
    "Thank you for the music, that makes me cry  \n",
    "  \n",
    "And you moving my bad as me, ah-hang wind in the hell  \n",
    "I was meant to be with you, I'll never be playing up\n",
    "```\n",
    "\n",
    "Bob Marley:\n",
    "\n",
    "```\n",
    "Mercy on judgment, we got so much  \n",
    "  \n",
    "Alcohol, cry, cry, cry  \n",
    "Why don't try to find our own  \n",
    "I want to know, Lord, I wanna give you  \n",
    "Just saving it, learned  \n",
    "Is there any more?  \n",
    "  \n",
    "All that damage done  \n",
    "That's all reason, don't worry  \n",
    "Need a hammer  \n",
    "I need you more and more  \n",
    "```\n",
    "\n",
    "Coldplay:\n",
    "\n",
    "```\n",
    "Look at the stars  \n",
    "Into life matter where you lay  \n",
    "Saying no doubt  \n",
    "I don't want to fly  \n",
    "In my dreams and fight today\n",
    "\n",
    "I will fall for you  \n",
    "  \n",
    "All I know  \n",
    "And I want you to stay  \n",
    "Into the night  \n",
    "  \n",
    "I want to live waiting  \n",
    "With my love and always  \n",
    "Have I wouldn't wasted  \n",
    "Would it hurt you\n",
    "```\n",
    "\n",
    "Kanye West:\n",
    "\n",
    "```\n",
    "I'm everywhere for you  \n",
    "The way that it couldn't stop  \n",
    "I mean it too late and love I made in the world  \n",
    "I told you so I took the studs full cold-stop  \n",
    "The hardest stressed growin'  \n",
    "The hustler raisin' on my tears  \n",
    "I know I'm true, one of your love\n",
    "```\n",
    "\n",
    "Looks pretty cool. We also noticed one interesting thing: the unconditional\n",
    "model usually performes better when you want to sample with a specified starting string.\n",
    "Our intuition is that when sampling from a conditional model with a specified starting string, \n",
    "we actually put two conditions on our model -- starting string and an artist compared to the one condition\n",
    "in the case of previous model that we explored. And we didn't have enough data to model that conditional\n",
    "distribution well (eavery artist have relatively limited number of songs).\n",
    "\n",
    "We are making the code and models available and you can sample songs from our trained models\n",
    "even without gpu as it is not really computationally demanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
