{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to generate lyrics and music with Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will train RNN character-level language model on lyrics dataset of\n",
    "most popular/recent artists.Having a trained model, we will sample a couple of\n",
    "songs which will be a funny mixture of different styles of different artists.\n",
    "After that we will update our model to become a Conditional Character-Level RNN,\n",
    "making it possible for us to sample songs conditioned on artist.\n",
    "And finally, we conclude by training our model on midi dataset of piano songs.\n",
    "While solving all these tasks, we will briefly explore some interesting concepts related to RNN\n",
    "training and inference like Character-Level RNN, Conditional Character-Level RNN,\n",
    "sampling from RNN, truncated backpropagation through time and checkpointing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character_level_model.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before choosing a model, let's have a closer look at our task at hand.\n",
    "\n",
    "Our language model is defined on a character level. We will create a dictionary which will contain\n",
    "all english characters plus some special symbols, like period, comma, and end-of-line symbol. Each charecter will be represented as one-hot-encoded tensor. For more information about character-level models and examples, I recommend [this resource](https://github.com/spro/practical-pytorch). We could have\n",
    "used a more advanced word-level model, but we will keep it simple for now.\n",
    "\n",
    "Having characters, we can now form sequences of characters. We can generate sentences even now just by\n",
    "randomly sampling character after character with a fixed probability $p(any~letter)=\\frac{1}{dictionary~size}$.\n",
    "That's the most simple character level language model. Can we do better than this? Yes, we can compute the probabily of occurance of each letter from our training corpus (number of times a letter occures divided by the size of our dataset) and randomly sample letter using these probabilities. This model is better but it totally ignores the relative positional aspect of each letter.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
